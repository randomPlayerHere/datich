{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f88168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuper/projects/datich/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define paths\n",
    "base_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "adapter_path = \"./binaries/qwen_lora\" # Path to your unzipped downloaded folder\n",
    "\n",
    "# 2. Load Tokenizer (saved along with your LoRA weights)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da836f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 290/290 [00:01<00:00, 258.86it/s, Materializing param=model.norm.weight]                              \n",
      "\u001b[1mQwen2ForSequenceClassification LOAD REPORT\u001b[0m from: Qwen/Qwen2.5-0.5B\n",
      "Key          | Status  | \n",
      "-------------+---------+-\n",
      "score.weight | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForSequenceClassification(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=896, out_features=6, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=896, out_features=6, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Load Base Model\n",
    "# IMPORTANT: Use the exact same torch_dtype you used during training (bfloat16 or float16)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    num_labels=6,\n",
    "    problem_type=\"regression\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 \n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 4. Snap the LoRA adapters onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.eval() # Lock the model into evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c335c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The order must match the target_columns from your training script exactly\n",
    "emotions = [\n",
    "    'sadness', 'anxiety', 'rumination', 'self_focus', \n",
    "    'hopelessness', 'emotional_volatility'\n",
    "]\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device (GPU/CPU) as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run the forward pass without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Extract the 6 regression scores from the model's output\n",
    "        raw_scores = outputs.logits[0].float().cpu().numpy()\n",
    "\n",
    "    # Enforce the 0.0 to 1.0 bounds\n",
    "    clipped_scores = np.clip(raw_scores, 0.0, 1.0)\n",
    "\n",
    "    # Zip the emotion names with their scores into a clean JSON-like dictionary\n",
    "    results = {\n",
    "        emotion: round(float(score), 3) \n",
    "        for emotion, score in zip(emotions, clipped_scores)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2821cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.578, 'anxiety': 0.0, 'rumination': 0.0, 'self_focus': 0.828, 'hopelessness': 0.0, 'emotional_volatility': 0.439}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"I want to die.\"\n",
    "\n",
    "predictions = analyze_text(sample_text)\n",
    "print(predictions)\n",
    "\n",
    "# Expected Output shape: \n",
    "# {'sadness': 0.812, 'anxiety': 0.945, 'rumination': 0.88, 'self_focus': 0.72, 'hopelessness': 0.65, 'emotional_volatility': 0.41}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171de61",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108eb013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sadness</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>rumination</th>\n",
       "      <th>self_focus</th>\n",
       "      <th>hopelessness</th>\n",
       "      <th>emotional_volatility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.793498</td>\n",
       "      <td>0.427534</td>\n",
       "      <td>0.686996</td>\n",
       "      <td>0.855830</td>\n",
       "      <td>0.778297</td>\n",
       "      <td>0.031563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756183</td>\n",
       "      <td>0.307626</td>\n",
       "      <td>0.135779</td>\n",
       "      <td>0.786356</td>\n",
       "      <td>0.652638</td>\n",
       "      <td>0.027205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.190433</td>\n",
       "      <td>0.586243</td>\n",
       "      <td>0.296508</td>\n",
       "      <td>0.559637</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.038687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.705457</td>\n",
       "      <td>0.556982</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.807384</td>\n",
       "      <td>0.593258</td>\n",
       "      <td>0.623114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sadness   anxiety  rumination  self_focus  hopelessness  \\\n",
       "cluster_id                                                             \n",
       "0           0.793498  0.427534    0.686996    0.855830      0.778297   \n",
       "1           0.756183  0.307626    0.135779    0.786356      0.652638   \n",
       "2           0.190433  0.586243    0.296508    0.559637      0.106250   \n",
       "3           0.705457  0.556982    0.483146    0.807384      0.593258   \n",
       "\n",
       "            emotional_volatility  \n",
       "cluster_id                        \n",
       "0                       0.031563  \n",
       "1                       0.027205  \n",
       "2                       0.038687  \n",
       "3                       0.623114  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load the scores your Qwen model generated for your dataset\n",
    "df = pd.read_csv(\"./data/final/final_dataset.csv\")\n",
    "features = ['sadness', 'anxiety', 'rumination', 'self_focus', 'hopelessness', 'emotional_volatility']\n",
    "X = df[features]\n",
    "\n",
    "# 2. Scale the data\n",
    "# Standardization is mandatory for distance-based clustering algorithms\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Apply K-Means Clustering\n",
    "# Let's assume we want to find 4 distinct profiles\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "df['cluster_id'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Profile the Centroids\n",
    "# This calculates the average score for each emotion within each cluster\n",
    "cluster_centroids = df.groupby('cluster_id')[features].mean()\n",
    "display(cluster_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f3c1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datich_scaler.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Define the mapping based on our interpretation\n",
    "cluster_mapping = {\n",
    "    0: \"Severe Distress / Depressive Profile\",\n",
    "    1: \"Passive Sadness / Apathy\",\n",
    "    2: \"Baseline / Mild Anxiety\",\n",
    "    3: \"Emotionally Volatile / Dysregulated\"\n",
    "}\n",
    "\n",
    "# 2. Save the K-Means model and the Scaler to your local directory\n",
    "joblib.dump(kmeans, \"datich_kmeans_model.pkl\")\n",
    "joblib.dump(scaler, \"datich_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b0f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
