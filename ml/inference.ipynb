{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f88168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuper/projects/datich/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f647247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define paths\n",
    "base_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "adapter_path = \"./qwen_lora\" # Path to your unzipped downloaded folder\n",
    "\n",
    "# 2. Load Tokenizer (saved along with your LoRA weights)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da836f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 290/290 [00:01<00:00, 258.86it/s, Materializing param=model.norm.weight]                              \n",
      "\u001b[1mQwen2ForSequenceClassification LOAD REPORT\u001b[0m from: Qwen/Qwen2.5-0.5B\n",
      "Key          | Status  | \n",
      "-------------+---------+-\n",
      "score.weight | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForSequenceClassification(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=896, out_features=6, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=896, out_features=6, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Load Base Model\n",
    "# IMPORTANT: Use the exact same torch_dtype you used during training (bfloat16 or float16)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    num_labels=6,\n",
    "    problem_type=\"regression\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 \n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 4. Snap the LoRA adapters onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.eval() # Lock the model into evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c335c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The order must match the target_columns from your training script exactly\n",
    "emotions = [\n",
    "    'sadness', 'anxiety', 'rumination', 'self_focus', \n",
    "    'hopelessness', 'emotional_volatility'\n",
    "]\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device (GPU/CPU) as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run the forward pass without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Extract the 6 regression scores from the model's output\n",
    "        raw_scores = outputs.logits[0].float().cpu().numpy()\n",
    "\n",
    "    # Enforce the 0.0 to 1.0 bounds\n",
    "    clipped_scores = np.clip(raw_scores, 0.0, 1.0)\n",
    "\n",
    "    # Zip the emotion names with their scores into a clean JSON-like dictionary\n",
    "    results = {\n",
    "        emotion: round(float(score), 3) \n",
    "        for emotion, score in zip(emotions, clipped_scores)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2821cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.656, 'anxiety': 0.236, 'rumination': 0.385, 'self_focus': 0.559, 'hopelessness': 0.535, 'emotional_volatility': 0.0}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"I've been pacing around my room for hours replaying that conversation. I just can't shake the feeling that I ruined everything and there's no way to fix it.\"\n",
    "\n",
    "predictions = analyze_text(sample_text)\n",
    "print(predictions)\n",
    "\n",
    "# Expected Output shape: \n",
    "# {'sadness': 0.812, 'anxiety': 0.945, 'rumination': 0.88, 'self_focus': 0.72, 'hopelessness': 0.65, 'emotional_volatility': 0.41}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load the scores your Qwen model generated for your dataset\n",
    "df = pd.read_csv(\"qwen_generated_scores.csv\")\n",
    "features = ['sadness', 'anxiety', 'rumination', 'self_focus', 'hopelessness', 'emotional_volatility']\n",
    "X = df[features]\n",
    "\n",
    "# 2. Scale the data\n",
    "# Standardization is mandatory for distance-based clustering algorithms\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Apply K-Means Clustering\n",
    "# Let's assume we want to find 4 distinct profiles\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "df['cluster_id'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Profile the Centroids\n",
    "# This calculates the average score for each emotion within each cluster\n",
    "cluster_centroids = df.groupby('cluster_id')[features].mean()\n",
    "display(cluster_centroids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
